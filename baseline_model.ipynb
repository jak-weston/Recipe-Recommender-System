{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "63f8f28d-9e4d-44b5-9712-159fb3c67c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in /home/cepeters/.local/lib/python3.11/site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from kagglehub) (24.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Using pip 24.0 from /opt/conda/lib/python3.11/site-packages/pip (python 3.11)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.13.0\n",
      "  Obtaining dependency information for tensorflow==2.13.0 from https://files.pythonhosted.org/packages/ed/30/310fee0477ce46f722c561dd7e21eebca0d1d29bdb3cf4a2335b845fbba4/tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for gast<=0.4.0,>=0.2.1 from https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/6f/c6/539660516ea7db7bc3d39e07154512ae807961b14ec6b5b0c58d15657ff1/grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/e1/89/118c3255d6ff2db33b062ec996a762d99ae50c21f54a8a6047ae8eda1b9f/h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for numpy<=1.24.3,>=1.22 from https://files.pythonhosted.org/packages/82/19/321d369ede7458500f59151101470129d14f3b6768bb9b99bb7156f526b5/numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for packaging from https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl.metadata\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/05/a6/094a2640be576d760baa34c902dcb8199d89bce9ed7dd7a6af74dcbbd62d/protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting setuptools (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/55/21/47d163f615df1d30c094f6c8bbb353619274edccf0327b185cc2493c2c33/setuptools-75.6.0-py3-none-any.whl.metadata\n",
      "  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for six>=1.12.0 from https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorboard<2.14,>=2.13 from https://files.pythonhosted.org/packages/67/f2/e8be5599634ff063fa2c59b7b51636815909d5140a26df9f02ce5d99b81a/tensorboard-2.13.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/7f/be/df630c387a0a054815d60be6a97eb4e8f17385d5d6fe660e1c02750062b4/termcolor-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for typing-extensions<4.6.0,>=3.6.6 from https://files.pythonhosted.org/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for wrapt>=1.11.0 from https://files.pythonhosted.org/packages/55/b5/698bd0bf9fbb3ddb3a2feefbb7ad0dea1205f5d7d05b9cbab54f5db731aa/wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7, <3.11'): https://files.pythonhosted.org/packages/fc/eb/2d62cc37fda071a12ba043f4fed34786e2c6c3fd725b054c231c6024ca0c/tensorflow_io_gcs_filesystem-0.28.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from https://pypi.org/simple/tensorflow-io-gcs-filesystem/) (requires-python:>=3.7, <3.11)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/66/7f/e36ae148c2f03d61ca1bff24bc13a0fef6d6825c966abef73fc6f880a23b/tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl.metadata\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/2d/9a/3d5087d27865c2f0431b942b5c4500b7d1b744dd3262fdc973a4c39d099e/google_auth-2.36.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for google-auth-oauthlib<1.1,>=0.5 from https://files.pythonhosted.org/packages/4a/07/8d9a8186e6768b55dfffeb57c719bc03770cf8a970a074616ae6f9e26a57/google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/3f/08/83871f3c50fc983b88547c196d11cf8c3340e37c32d2e9d6152abe2c61f7/Markdown-3.7-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/52/24/ab44c871b0f07f491e5d2ad12c9bd7358e527510618cb1b803a88e986db1/werkzeug-3.1.3-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for pyasn1-modules>=0.2.1 from https://files.pythonhosted.org/packages/77/89/bc88a6711935ba795a679ea6ebee07e128050d6382eaa35a0a47c8032bdc/pyasn1_modules-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/eb/5b/6f10bad0f6461fa272bfbbdf5d0023b5fb9bc6217c92bf068fa5a99820f5/charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/f1/a4/aefb044a2cd8d7334c8a47d3fb2c9f328ac48cb349468cc31c20b539305f/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for pyasn1<0.7.0,>=0.4.6 from https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.2/524.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.6/142.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  changing mode of /home/cepeters/.local/bin/wheel to 775\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/f2py to 775\n",
      "  changing mode of /home/cepeters/.local/bin/f2py3 to 775\n",
      "  changing mode of /home/cepeters/.local/bin/f2py3.11 to 775\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.11 are installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/markdown_py to 775\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/normalizer to 775\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/pyrsa-decrypt to 775\n",
      "  changing mode of /home/cepeters/.local/bin/pyrsa-encrypt to 775\n",
      "  changing mode of /home/cepeters/.local/bin/pyrsa-keygen to 775\n",
      "  changing mode of /home/cepeters/.local/bin/pyrsa-priv2pub to 775\n",
      "  changing mode of /home/cepeters/.local/bin/pyrsa-sign to 775\n",
      "  changing mode of /home/cepeters/.local/bin/pyrsa-verify to 775\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/google-oauthlib-tool to 775\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/tensorboard to 775\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/cepeters/.local/bin/estimator_ckpt_converter to 775\n",
      "  changing mode of /home/cepeters/.local/bin/import_pb_to_tensorboard to 775\n",
      "  changing mode of /home/cepeters/.local/bin/saved_model_cli to 775\n",
      "  changing mode of /home/cepeters/.local/bin/tensorboard to 775\n",
      "  changing mode of /home/cepeters/.local/bin/tf_upgrade_v2 to 775\n",
      "  changing mode of /home/cepeters/.local/bin/tflite_convert to 775\n",
      "  changing mode of /home/cepeters/.local/bin/toco to 775\n",
      "  changing mode of /home/cepeters/.local/bin/toco_from_protos to 775\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/cepeters/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.2.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "sqlalchemy 2.0.29 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.36.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 idna-3.10 keras-2.13.1 libclang-18.1.1 markdown-3.7 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.4.0 packaging-24.2 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 setuptools-75.6.0 six-1.16.0 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 typing-extensions-4.5.0 urllib3-2.2.3 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub --upgrade\n",
    "!pip install -Iv tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6fbd7100-ad97-4b14-a8f3-50690ca5e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc9392e-b281-4982-b683-99bd4b7c9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/cepeters/.cache/kagglehub/datasets/shuyangli94/food-com-recipes-and-user-interactions/versions/2\n",
      "Files in the dataset: ['interactions_validation.csv', 'PP_users.csv', 'RAW_recipes.csv', 'interactions_test.csv', 'interactions_train.csv', 'ingr_map.pkl', 'PP_recipes.csv', 'RAW_interactions.csv']\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List all files in the dataset directory\n",
    "dataset_files = os.listdir(path)\n",
    "print(\"Files in the dataset:\", dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ca66d0-9b01-4e3d-bafa-a18b988e514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading interactions_validation.csv into a DataFrame...\n",
      "Loaded interactions_validation.csv with 7023 rows and 6 columns.\n",
      "Loading PP_users.csv into a DataFrame...\n",
      "Loaded PP_users.csv with 25076 rows and 6 columns.\n",
      "Loading RAW_recipes.csv into a DataFrame...\n",
      "Loaded RAW_recipes.csv with 231637 rows and 12 columns.\n",
      "Loading interactions_test.csv into a DataFrame...\n",
      "Loaded interactions_test.csv with 12455 rows and 6 columns.\n",
      "Loading interactions_train.csv into a DataFrame...\n",
      "Loaded interactions_train.csv with 698901 rows and 6 columns.\n",
      "Loading PP_recipes.csv into a DataFrame...\n",
      "Loaded PP_recipes.csv with 178265 rows and 8 columns.\n",
      "Loading RAW_interactions.csv into a DataFrame...\n",
      "Loaded RAW_interactions.csv with 1132367 rows and 5 columns.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store DataFrames for each useful file\n",
    "dataframes = {}\n",
    "\n",
    "# Load each useful CSV file into a DataFrame dictionary\n",
    "for file_name in dataset_files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    if file_name.endswith('.csv') and os.path.exists(file_path):\n",
    "        print(f\"Loading {file_name} into a DataFrame...\")\n",
    "        df_name = file_name.split('.')[0]  # Use filename without extension as key\n",
    "        dataframes[df_name] = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {file_name} with {dataframes[df_name].shape[0]} rows and {dataframes[df_name].shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48c4712-e891-4912-9400-6778c70ae809",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_interactions_df = dataframes.get('RAW_interactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cda476a-2658-41c7-94e4-c9c06eb1f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split interactions into training and testing dataframes\n",
    "shuffled_raw_interactions_df = raw_interactions_df.sample(frac=1).reset_index(drop=True)\n",
    "train_raw_interactions_df = shuffled_raw_interactions_df[0:500000]\n",
    "test_raw_interactions_df = shuffled_raw_interactions_df[500000:600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "825bc169-cd63-458d-8041-723afdab3294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.410302"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through training set to find values\n",
    "globalAvg = 0\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerRecipe = defaultdict(list)\n",
    "\n",
    "for i in range(train_raw_interactions_df.shape[0]):\n",
    "    row = train_raw_interactions_df.iloc[i]\n",
    "    rating, user, recipe = row[\"rating\"], row[\"user_id\"], row[\"recipe_id\"]\n",
    "    globalAvg += rating\n",
    "    ratingsPerUser[user].append((recipe, rating))\n",
    "    ratingsPerRecipe[recipe].append((user, rating))\n",
    "\n",
    "globalAvg /= train_raw_interactions_df.shape[0]\n",
    "globalAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ea1a179-759e-41f8-b50a-117b37c8fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Using Global Averages) = 1.5887777994048145\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Find MSE of predicting global average every time\n",
    "MSE = 0\n",
    "for i in range(test_raw_interactions_df.shape[0]):\n",
    "    row = test_raw_interactions_df.iloc[i]\n",
    "    error = (row[\"rating\"] - globalAvg) ** 2\n",
    "    MSE += error\n",
    "\n",
    "MSE /= test_raw_interactions_df.shape[0]\n",
    "\n",
    "print(\"Test MSE (Using Global Averages) = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ded8b019-c64c-4b86-bae9-6162ce65761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Using User Averages) = 1.5933343466580254\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Find MSE of predicting user average, or global average if there is none\n",
    "MSE = 0\n",
    "for i in range(test_raw_interactions_df.shape[0]):\n",
    "    row = test_raw_interactions_df.iloc[i]\n",
    "    if row[\"user_id\"] in ratingsPerUser:\n",
    "        userRatings = [r[1] for r in ratingsPerUser[row[\"user_id\"]]]\n",
    "        avgUserRating = sum(userRatings)/len(userRatings)\n",
    "        error = (row[\"rating\"] - avgUserRating) ** 2\n",
    "    else:\n",
    "        error = (row[\"rating\"] - globalAvg) ** 2\n",
    "    MSE += error\n",
    "\n",
    "MSE /= test_raw_interactions_df.shape[0]\n",
    "\n",
    "print(\"Test MSE (Using User Averages) = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f5e76f0-9f7c-427c-8363-accdda29c1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Using Recipe Averages) = 1.8437603678229684\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Find MSE of predicting recipe average, or global average if there is none\n",
    "MSE = 0\n",
    "for i in range(test_raw_interactions_df.shape[0]):\n",
    "    row = test_raw_interactions_df.iloc[i]\n",
    "    if row[\"recipe_id\"] in ratingsPerRecipe:\n",
    "        recipeRatings = [r[1] for r in ratingsPerRecipe[row[\"recipe_id\"]]]\n",
    "        avgRecipeRating = sum(recipeRatings)/len(recipeRatings)\n",
    "        error = (row[\"rating\"] - avgRecipeRating) ** 2\n",
    "    else:\n",
    "        error = (row[\"rating\"] - globalAvg) ** 2\n",
    "    MSE += error\n",
    "\n",
    "MSE /= test_raw_interactions_df.shape[0]\n",
    "\n",
    "print(\"Test MSE (Using Recipe Averages) = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4453618d-b9e5-4c64-a14d-54be18af9cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Using User & Recipe Averages) = 1.5781550707033012\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Find MSE of predicting user average or recipe average (whichever has more), or global average if there is neither\n",
    "MSE = 0\n",
    "for i in range(test_raw_interactions_df.shape[0]):\n",
    "    row = test_raw_interactions_df.iloc[i]\n",
    "    user, recipe = row[\"user_id\"], row[\"recipe_id\"]\n",
    "    if user in ratingsPerUser and (recipe not in ratingsPerRecipe or len(ratingsPerRecipe[recipe]) <= len(ratingsPerUser[user])):\n",
    "        userRatings = [r[1] for r in ratingsPerUser[user]]\n",
    "        avgRating = sum(userRatings) / len(userRatings)\n",
    "        error = (row[\"rating\"] - avgRating) ** 2\n",
    "    elif recipe in ratingsPerRecipe and (user not in ratingsPerUser or len(ratingsPerRecipe[recipe]) >= len(ratingsPerUser[user])):\n",
    "        recipeRatings = [r[1] for r in ratingsPerRecipe[recipe]]\n",
    "        avgRating = sum(recipeRatings) / len(recipeRatings)\n",
    "        error = (row[\"rating\"] - avgRating) ** 2\n",
    "    else:\n",
    "        error = (row[\"rating\"] - globalAvg) ** 2\n",
    "    MSE += error\n",
    "\n",
    "MSE /= test_raw_interactions_df.shape[0]\n",
    "\n",
    "print(\"Test MSE (Using User & Recipe Averages) = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "201da63e-c9e6-453a-8f2e-8fdd0984acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration function for our model...\n",
    "def iterate(lamb):\n",
    "    newAlpha = 0\n",
    "    for i in range(train_raw_interactions_df.shape[0]):\n",
    "        row = train_raw_interactions_df.iloc[i]\n",
    "        rating = row[\"rating\"]\n",
    "        user = row[\"user_id\"]\n",
    "        recipe = row[\"recipe_id\"]\n",
    "        newAlpha += rating - (betaU[user] + betaR[recipe])\n",
    "    alpha = newAlpha / train_raw_interactions_df.shape[0]\n",
    "    for user in ratingsPerUser:\n",
    "        newBetaU = 0\n",
    "        for recipe,rating in ratingsPerUser[user]:\n",
    "            newBetaU += rating - (alpha + betaR[recipe])\n",
    "        betaU[user] = newBetaU / (lamb + len(ratingsPerUser[user]))\n",
    "    for recipe in ratingsPerRecipe:\n",
    "        newBetaR = 0\n",
    "        for user,rating in ratingsPerRecipe[recipe]:\n",
    "            newBetaR += rating - (alpha + betaU[user])\n",
    "        betaR[recipe] = newBetaR / (lamb + len(ratingsPerRecipe[recipe]))\n",
    "    mse = 0\n",
    "    for i in range(train_raw_interactions_df.shape[0]):\n",
    "        row = train_raw_interactions_df.iloc[i]\n",
    "        rating = row[\"rating\"]\n",
    "        user = row[\"user_id\"]\n",
    "        recipe = row[\"recipe_id\"]\n",
    "        prediction = alpha + betaU[user] + betaR[recipe]\n",
    "        mse += (rating - prediction)**2\n",
    "    regularizer = 0\n",
    "    for u in betaU:\n",
    "        regularizer += betaU[u]**2\n",
    "    for r in betaR:\n",
    "        regularizer += betaR[r]**2\n",
    "    mse /= train_raw_interactions_df.shape[0]\n",
    "    return mse, (mse * train_raw_interactions_df.shape[0]) + lamb*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00a2abf5-838c-45b0-8ded-1fdd291c5a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 1 iteration = 426413.9127441286\n",
      "MSE after 1 iteration = 0.5813430435560253\n",
      "Objective after 2 iterations = 413436.41958465974\n",
      "MSE after 2 iterations = 0.5813672803234096\n",
      "Objective after 3 iterations = 411638.79800325056\n",
      "MSE after 3 iterations = 0.582191948248583\n",
      "Objective after 4 iterations = 410769.1507638578\n",
      "MSE after 4 iterations = 0.5819238398581439\n",
      "Objective after 5 iterations = 410070.884568783\n",
      "MSE after 5 iterations = 0.5813452785917188\n",
      "Objective after 6 iterations = 409459.0172681245\n",
      "MSE after 6 iterations = 0.5807216913188809\n",
      "Objective after 7 iterations = 408919.7374748698\n",
      "MSE after 7 iterations = 0.580130986536033\n",
      "Objective after 8 iterations = 408447.90265646734\n",
      "MSE after 8 iterations = 0.579594780258637\n",
      "Objective after 9 iterations = 408038.8991968699\n",
      "MSE after 9 iterations = 0.5791172015994022\n",
      "Objective after 10 iterations = 407687.58645509905\n",
      "MSE after 10 iterations = 0.5786964237172423\n",
      "Objective after 11 iterations = 407388.37232360046\n",
      "MSE after 11 iterations = 0.5783283901665406\n",
      "Objective after 12 iterations = 407135.4777816542\n",
      "MSE after 12 iterations = 0.5780082088822496\n",
      "Objective after 13 iterations = 406923.19422726624\n",
      "MSE after 13 iterations = 0.5777307766655505\n",
      "Objective after 14 iterations = 406746.08385597816\n",
      "MSE after 14 iterations = 0.5774911033557085\n",
      "Objective after 15 iterations = 406599.11489014217\n",
      "MSE after 15 iterations = 0.5772844910010685\n",
      "Objective after 16 iterations = 406477.7385686428\n",
      "MSE after 16 iterations = 0.5771066291999934\n",
      "Objective after 17 iterations = 406377.91993951815\n",
      "MSE after 17 iterations = 0.5769536370308354\n",
      "Objective after 18 iterations = 406296.13505784416\n",
      "MSE after 18 iterations = 0.5768220701098065\n",
      "Objective after 19 iterations = 406229.34570301627\n",
      "MSE after 19 iterations = 0.5767089054132414\n",
      "Objective after 20 iterations = 406174.9605348319\n",
      "MSE after 20 iterations = 0.5766115127958088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m iterations \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m objective \u001b[38;5;241m-\u001b[39m newObjective \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0001\u001b[39m:\n\u001b[1;32m     17\u001b[0m     mse, objective \u001b[38;5;241m=\u001b[39m newMSE, newObjective\n\u001b[0;32m---> 18\u001b[0m     newMSE, newObjective \u001b[38;5;241m=\u001b[39m \u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObjective after \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(iterations) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m iterations = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iterations \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m iteration = \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(newObjective))\n",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m, in \u001b[0;36miterate\u001b[0;34m(lamb)\u001b[0m\n\u001b[1;32m      3\u001b[0m newAlpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_raw_interactions_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_raw_interactions_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m     rating \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m     user \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1754\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3996\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3994\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3996\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3998\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3999\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:984\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    976\u001b[0m     block \u001b[38;5;241m=\u001b[39m new_block(\n\u001b[1;32m    977\u001b[0m         result,\n\u001b[1;32m    978\u001b[0m         placement\u001b[38;5;241m=\u001b[39mbp,\n\u001b[1;32m    979\u001b[0m         ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    980\u001b[0m         refs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrefs,\n\u001b[1;32m    981\u001b[0m     )\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SingleBlockManager(block, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 984\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43minterleaved_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# TODO: use object dtype as workaround for non-performant\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m#  EA.__setitem__ methods. (primarily ArrowExtensionArray.__setitem__\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m#  when iteratively setting individual values)\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m#  https://github.com/pandas-dev/pandas/pull/54508#issuecomment-1675827918\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/base.py:394\u001b[0m, in \u001b[0;36minterleaved_dtype\u001b[0;34m(dtypes)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_common_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1474\u001b[0m, in \u001b[0;36mfind_common_type\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(lib\u001b[38;5;241m.\u001b[39mis_np_dtype(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mmax\u001b[39m(types))\n\u001b[0;32m-> 1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(lib\u001b[38;5;241m.\u001b[39mis_np_dtype(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mmax\u001b[39m(types))\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# don't mix bool / int or float or complex\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;66;03m# this is different from numpy, which casts bool with float/int as int\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1474\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(lib\u001b[38;5;241m.\u001b[39mis_np_dtype(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mmax\u001b[39m(types))\n\u001b[0;32m-> 1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(lib\u001b[38;5;241m.\u001b[39mis_np_dtype(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mmax\u001b[39m(types))\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# don't mix bool / int or float or complex\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;66;03m# this is different from numpy, which casts bool with float/int as int\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model generation...\n",
    "# Values:\n",
    "betaU = {}\n",
    "betaR = {}\n",
    "for u in ratingsPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for r in ratingsPerRecipe:\n",
    "    betaR[r] = 0\n",
    "\n",
    "alpha = globalAvg\n",
    "\n",
    "# Iterating:\n",
    "iterations = 0\n",
    "mse, objective, newMSE, newObjective = 0, 0, 0, 0\n",
    "while iterations < 10 or objective - newObjective > 0.0001:\n",
    "    mse, objective = newMSE, newObjective\n",
    "    newMSE, newObjective = iterate(1)\n",
    "    iterations += 1\n",
    "    print(\"Objective after \"\n",
    "        + str(iterations) + (\" iterations = \" if iterations != 1 else \" iteration = \") + str(newObjective))\n",
    "    print(\"MSE after \"\n",
    "        + str(iterations) + (\" iterations = \" if iterations != 1 else \" iteration = \") + str(newMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1d4c023-2e8b-47b3-b443-3441bf702c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = 1.6571930897383143\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "MSE = 0\n",
    "for i in range(test_raw_interactions_df.shape[0]):\n",
    "    row = test_raw_interactions_df.iloc[i]\n",
    "    rating = row[\"rating\"]\n",
    "    user = row[\"user_id\"]\n",
    "    recipe = row[\"recipe_id\"]\n",
    "    bu = 0\n",
    "    br = 0\n",
    "    if user in betaU:\n",
    "        bu = betaU[user]\n",
    "    if recipe in betaR:\n",
    "        br = betaR[recipe]\n",
    "    prediction = alpha + bu + br\n",
    "    MSE += (rating - prediction) ** 2\n",
    "\n",
    "MSE /= test_raw_interactions_df.shape[0]\n",
    "print(\"Test MSE = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eef12c42-0f7f-4e3f-8f1c-5561ad0d7620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132367"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userIDs = {}\n",
    "recipeIDs = {}\n",
    "interactions = []\n",
    "\n",
    "for i in range(raw_interactions_df.shape[0]):\n",
    "    row = raw_interactions_df.iloc[i]\n",
    "    user = row['user_id']\n",
    "    recipe = row['recipe_id']\n",
    "    rating = row['rating']\n",
    "    if not user in userIDs: userIDs[user] = len(userIDs)\n",
    "    if not recipe in recipeIDs: recipeIDs[recipe] = len(recipeIDs)\n",
    "    interactions.append((user,recipe,rating))\n",
    "\n",
    "random.shuffle(interactions)\n",
    "len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7516a62-0e51-42e5-a77e-f7a0405020f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = int(len(interactions) * 0.9)\n",
    "nTest = len(interactions) - nTrain\n",
    "interactionsTrain = interactions[:nTrain]\n",
    "interactionsTest = interactions[nTrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7ba1403-c554-4480-80f0-94eef68378b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipesPerUser = defaultdict(list)\n",
    "usersPerRecipe = defaultdict(list)\n",
    "for user,recipe,rating in interactionsTrain:\n",
    "    recipesPerUser[user].append(recipe)\n",
    "    usersPerRecipe[recipe].append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ced13ddc-a81d-435f-ba9d-6bc36f7803d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = sum([rating for _,_,rating in interactionsTrain]) / len(interactionsTrain)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "0b309bba-e402-43a2-8e5e-f3e6c2925e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.cast(tf.Variable(mu), dtype=tf.float32)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaR = tf.Variable(tf.random.normal([len(recipeIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaR = tf.Variable(tf.random.normal([len(recipeIDs),K],stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, r):\n",
    "        prediction = self.alpha + self.betaU[u] + self.betaR[r] +\\\n",
    "            tf.tensordot(self.gammaU[u], self.gammaR[r], 1)\n",
    "        return prediction\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "                            tf.reduce_sum(self.betaR**2) +\\\n",
    "                            tf.reduce_sum(self.gammaU**2) +\\\n",
    "                            tf.reduce_sum(self.gammaR**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleR):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_r = tf.nn.embedding_lookup(self.betaR, r)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_r = tf.nn.embedding_lookup(self.gammaR, r)\n",
    "        prediction = self.alpha + beta_u + beta_r + tf.reduce_sum(tf.multiply(gamma_u, gamma_r), 1)\n",
    "        return prediction\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleUser, sampleRecipe, sampleRating):\n",
    "        prediction = self.predictSample(sampleUser, sampleRecipe)\n",
    "        rating = tf.convert_to_tensor(sampleRating, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(prediction - rating) / len(sampleRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ec4a7d7d-0009-4b41-95d1-1e62757b7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLFM = LatentFactorModel(mu, 5, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3ebcdea9-38b7-48e8-93d5-02d147adf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleUser, sampleRecipe, sampleRating = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            user,recipe,rating = random.choice(interactions)\n",
    "            sampleUser.append(userIDs[user])\n",
    "            sampleRecipe.append(recipeIDs[recipe])\n",
    "            sampleRating.append(rating)\n",
    "\n",
    "        loss = model(sampleUser,sampleRecipe,sampleRating)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d876534e-789b-4d07-b67b-25f8ba6db00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, objective = 0.78203887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[319], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mtrainingStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelLFM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteractionsTrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, objective = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(obj))\n",
      "Cell \u001b[0;32mIn[318], line 11\u001b[0m, in \u001b[0;36mtrainingStep\u001b[0;34m(model, interactions)\u001b[0m\n\u001b[1;32m      8\u001b[0m         sampleRecipe\u001b[38;5;241m.\u001b[39mappend(recipeIDs[recipe])\n\u001b[1;32m      9\u001b[0m         sampleRating\u001b[38;5;241m.\u001b[39mappend(rating)\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampleUser\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampleRecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampleRating\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreg()\n\u001b[1;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py:590\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    588\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/base_layer.py:1144\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autocast:\n\u001b[0;32m-> 1144\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1148\u001b[0m ):\n\u001b[1;32m   1149\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/base_layer.py:2750\u001b[0m, in \u001b[0;36mLayer._maybe_cast_inputs\u001b[0;34m(self, inputs, input_list)\u001b[0m\n\u001b[1;32m   2743\u001b[0m compute_dtype_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   2744\u001b[0m should_autocast \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autocast\n\u001b[1;32m   2746\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compute_dtype_object\n\u001b[1;32m   2747\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compute_dtype_object\u001b[38;5;241m.\u001b[39mis_floating\n\u001b[1;32m   2748\u001b[0m )\n\u001b[0;32m-> 2750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_autocast \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2751\u001b[0m     \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_cast_single_input, input_list)\n\u001b[1;32m   2752\u001b[0m ):\n\u001b[1;32m   2753\u001b[0m     \u001b[38;5;66;03m# Only perform expensive `nest` operation when needed.\u001b[39;00m\n\u001b[1;32m   2754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_single_input, inputs)\n\u001b[1;32m   2755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/base_layer.py:2763\u001b[0m, in \u001b[0;36mLayer._should_cast_single_input\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_should_cast_single_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   2759\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _AUTOCAST_TYPES):\n\u001b[1;32m   2760\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   2761\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   2762\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m-> 2763\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241m.\u001b[39mis_floating\n\u001b[1;32m   2764\u001b[0m         )\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:364\u001b[0m, in \u001b[0;36m_EagerTensorBase.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    366\u001b[0m   \u001b[38;5;66;03m# Note: using the intern table directly here as this is\u001b[39;00m\n\u001b[1;32m    367\u001b[0m   \u001b[38;5;66;03m# performance-sensitive in some models.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39m_INTERN_TABLE[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datatype_enum()]  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    obj = trainingStep(modelLFM, interactionsTrain)\n",
    "    print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0e91bb38-7b28-45a1-ae0f-d0e99808ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 113237\n",
      "10000 / 113237\n",
      "20000 / 113237\n",
      "30000 / 113237\n",
      "40000 / 113237\n",
      "50000 / 113237\n",
      "60000 / 113237\n",
      "70000 / 113237\n",
      "80000 / 113237\n",
      "90000 / 113237\n",
      "100000 / 113237\n",
      "110000 / 113237\n",
      "Test MSE = tf.Tensor(1.5663366, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "MSE = 0\n",
    "for (i,(user,recipe,rating)) in enumerate(interactionsTest):\n",
    "    if i % 10000 == 0: print(i, \"/\", len(interactionsTest))\n",
    "    prediction = modelLFM.predict(userIDs[user], recipeIDs[recipe])\n",
    "    MSE += (rating - prediction) ** 2\n",
    "\n",
    "MSE /= len(interactionsTest)\n",
    "print(\"Test MSE = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f7f073f0-f18d-41ba-8f59-6e8ad8f96d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = tf.Tensor(1.593523, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "MSE = 0\n",
    "\n",
    "predictions = modelLFM.predictSample([interaction[0] for interaction in interactionsTest],[interaction[1] for interaction in interactionsTest])\n",
    "\n",
    "for i in range(len(interactionsTest)):\n",
    "    MSE += (interactionsTest[i][2] - predictions[i]) ** 2\n",
    "\n",
    "MSE /= len(interactionsTest)\n",
    "print(\"Test MSE = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd3502-2868-4f02-9303-c225aa1f0cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
