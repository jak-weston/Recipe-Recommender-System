{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff9e7cb-d1c8-4cde-9f73-949bdc51eca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in /home/jweston/.local/lib/python3.11/site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in /home/jweston/.local/lib/python3.11/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in /home/jweston/.local/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jweston/.local/lib/python3.11/site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jweston/.local/lib/python3.11/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jweston/.local/lib/python3.11/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jweston/.local/lib/python3.11/site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Using pip 24.0 from /opt/conda/lib/python3.11/site-packages/pip (python 3.11)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.13.0\n",
      "  Obtaining dependency information for tensorflow==2.13.0 from https://files.pythonhosted.org/packages/ed/30/310fee0477ce46f722c561dd7e21eebca0d1d29bdb3cf4a2335b845fbba4/tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for gast<=0.4.0,>=0.2.1 from https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl.metadata\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/6f/c6/539660516ea7db7bc3d39e07154512ae807961b14ec6b5b0c58d15657ff1/grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/e1/89/118c3255d6ff2db33b062ec996a762d99ae50c21f54a8a6047ae8eda1b9f/h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for numpy<=1.24.3,>=1.22 from https://files.pythonhosted.org/packages/82/19/321d369ede7458500f59151101470129d14f3b6768bb9b99bb7156f526b5/numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl.metadata\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for packaging from https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl.metadata\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/05/a6/094a2640be576d760baa34c902dcb8199d89bce9ed7dd7a6af74dcbbd62d/protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting setuptools (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/55/21/47d163f615df1d30c094f6c8bbb353619274edccf0327b185cc2493c2c33/setuptools-75.6.0-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for six>=1.12.0 from https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorboard<2.14,>=2.13 from https://files.pythonhosted.org/packages/67/f2/e8be5599634ff063fa2c59b7b51636815909d5140a26df9f02ce5d99b81a/tensorboard-2.13.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/7f/be/df630c387a0a054815d60be6a97eb4e8f17385d5d6fe660e1c02750062b4/termcolor-2.5.0-py3-none-any.whl.metadata\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for typing-extensions<4.6.0,>=3.6.6 from https://files.pythonhosted.org/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for wrapt>=1.11.0 from https://files.pythonhosted.org/packages/55/b5/698bd0bf9fbb3ddb3a2feefbb7ad0dea1205f5d7d05b9cbab54f5db731aa/wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "  Link requires a different Python (3.11.9 not in: '>=3.7, <3.11'): https://files.pythonhosted.org/packages/fc/eb/2d62cc37fda071a12ba043f4fed34786e2c6c3fd725b054c231c6024ca0c/tensorflow_io_gcs_filesystem-0.28.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from https://pypi.org/simple/tensorflow-io-gcs-filesystem/) (requires-python:>=3.7, <3.11)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/66/7f/e36ae148c2f03d61ca1bff24bc13a0fef6d6825c966abef73fc6f880a23b/tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl.metadata\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/2d/9a/3d5087d27865c2f0431b942b5c4500b7d1b744dd3262fdc973a4c39d099e/google_auth-2.36.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for google-auth-oauthlib<1.1,>=0.5 from https://files.pythonhosted.org/packages/4a/07/8d9a8186e6768b55dfffeb57c719bc03770cf8a970a074616ae6f9e26a57/google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/3f/08/83871f3c50fc983b88547c196d11cf8c3340e37c32d2e9d6152abe2c61f7/Markdown-3.7-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/52/24/ab44c871b0f07f491e5d2ad12c9bd7358e527510618cb1b803a88e986db1/werkzeug-3.1.3-py3-none-any.whl.metadata\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for pyasn1-modules>=0.2.1 from https://files.pythonhosted.org/packages/77/89/bc88a6711935ba795a679ea6ebee07e128050d6382eaa35a0a47c8032bdc/pyasn1_modules-0.4.1-py3-none-any.whl.metadata\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/eb/5b/6f10bad0f6461fa272bfbbdf5d0023b5fb9bc6217c92bf068fa5a99820f5/charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/f1/a4/aefb044a2cd8d7334c8a47d3fb2c9f328ac48cb349468cc31c20b539305f/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for pyasn1<0.7.0,>=0.4.6 from https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl.metadata\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.2 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  changing mode of /home/jweston/.local/bin/wheel to 775\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/f2py to 775\n",
      "  changing mode of /home/jweston/.local/bin/f2py3 to 775\n",
      "  changing mode of /home/jweston/.local/bin/f2py3.11 to 775\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.11 are installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/markdown_py to 775\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/normalizer to 775\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/pyrsa-decrypt to 775\n",
      "  changing mode of /home/jweston/.local/bin/pyrsa-encrypt to 775\n",
      "  changing mode of /home/jweston/.local/bin/pyrsa-keygen to 775\n",
      "  changing mode of /home/jweston/.local/bin/pyrsa-priv2pub to 775\n",
      "  changing mode of /home/jweston/.local/bin/pyrsa-sign to 775\n",
      "  changing mode of /home/jweston/.local/bin/pyrsa-verify to 775\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/google-oauthlib-tool to 775\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/tensorboard to 775\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  changing mode of /home/jweston/.local/bin/estimator_ckpt_converter to 775\n",
      "  changing mode of /home/jweston/.local/bin/import_pb_to_tensorboard to 775\n",
      "  changing mode of /home/jweston/.local/bin/saved_model_cli to 775\n",
      "  changing mode of /home/jweston/.local/bin/tensorboard to 775\n",
      "  changing mode of /home/jweston/.local/bin/tf_upgrade_v2 to 775\n",
      "  changing mode of /home/jweston/.local/bin/tflite_convert to 775\n",
      "  changing mode of /home/jweston/.local/bin/toco to 775\n",
      "  changing mode of /home/jweston/.local/bin/toco_from_protos to 775\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jweston/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 2.9.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "typeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.23.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "torch 2.2.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "sqlalchemy 2.0.29 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.36.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 idna-3.10 keras-2.13.1 libclang-18.1.1 markdown-3.7 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.4.0 packaging-24.2 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-2.32.3 requests-oauthlib-2.0.0 rsa-4.9 setuptools-75.6.0 six-1.16.0 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 typing-extensions-4.5.0 urllib3-2.2.3 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/jweston/.local/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jweston/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/jweston/.local/lib/python3.11/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jweston/.local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jweston/.local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub --upgrade\n",
    "!pip install -Iv tensorflow==2.13.0\n",
    "!pip install pandas\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2b4397-8614-43ac-9ac7-60d91bfe7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 06:05:19.657547: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 06:05:19.833711: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 06:05:19.835616: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jweston/.cache/kagglehub/datasets/shuyangli94/food-com-recipes-and-user-interactions/versions/2\n",
      "Files in the dataset: ['RAW_recipes.csv', 'PP_users.csv', 'interactions_test.csv', 'interactions_train.csv', 'PP_recipes.csv', 'ingr_map.pkl', 'interactions_validation.csv', 'RAW_interactions.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Dataset loading (original code)\n",
    "path = kagglehub.dataset_download(\"shuyangli94/food-com-recipes-and-user-interactions\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List all files in the dataset directory\n",
    "dataset_files = os.listdir(path)\n",
    "print(\"Files in the dataset:\", dataset_files)\n",
    "\n",
    "# Dictionary to store DataFrames for each useful file\n",
    "dataframes = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe587f9-acfd-4c72-8928-c6104c1d3b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAW_recipes.csv into a DataFrame...\n",
      "Loaded RAW_recipes.csv with 231637 rows and 12 columns.\n",
      "Loading PP_users.csv into a DataFrame...\n",
      "Loaded PP_users.csv with 25076 rows and 6 columns.\n",
      "Loading interactions_test.csv into a DataFrame...\n",
      "Loaded interactions_test.csv with 12455 rows and 6 columns.\n",
      "Loading interactions_train.csv into a DataFrame...\n",
      "Loaded interactions_train.csv with 698901 rows and 6 columns.\n",
      "Loading PP_recipes.csv into a DataFrame...\n",
      "Loaded PP_recipes.csv with 178265 rows and 8 columns.\n",
      "Loading interactions_validation.csv into a DataFrame...\n",
      "Loaded interactions_validation.csv with 7023 rows and 6 columns.\n",
      "Loading RAW_interactions.csv into a DataFrame...\n",
      "Loaded RAW_interactions.csv with 1132367 rows and 5 columns.\n"
     ]
    }
   ],
   "source": [
    "# Load each useful CSV file into a DataFrame dictionary\n",
    "for file_name in dataset_files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    if file_name.endswith('.csv') and os.path.exists(file_path):\n",
    "        print(f\"Loading {file_name} into a DataFrame...\")\n",
    "        df_name = file_name.split('.')[0]  # Use filename without extension as key\n",
    "        dataframes[df_name] = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {file_name} with {dataframes[df_name].shape[0]} rows and {dataframes[df_name].shape[1]} columns.\")\n",
    "\n",
    "raw_interactions_df = dataframes.get('RAW_interactions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e54d85-b9d6-4862-b4a8-a3aabf52c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate userIDs, recipeIDs, and put interactions into a set\n",
    "userIDs = {}\n",
    "recipeIDs = {}\n",
    "interactions = []\n",
    "\n",
    "for i in range(raw_interactions_df.shape[0]):\n",
    "    row = raw_interactions_df.iloc[i]\n",
    "    user = row['user_id']\n",
    "    recipe = row['recipe_id']\n",
    "    rating = row['rating']\n",
    "    if not user in userIDs: userIDs[user] = len(userIDs)\n",
    "    if not recipe in recipeIDs: recipeIDs[recipe] = len(recipeIDs)\n",
    "    interactions.append((user, recipe, rating))\n",
    "\n",
    "random.shuffle(interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eaa7b6-b30a-412d-b52d-f9cc616763b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Using Global Averages) = 1.6081370643884438\n"
     ]
    }
   ],
   "source": [
    "# Split interactions into training and testing sets\n",
    "# Dataset preparation (Ensure this runs first)\n",
    "random.shuffle(interactions)\n",
    "nTrain = int(len(interactions) * 0.9)\n",
    "interactionsTrain = interactions[:nTrain]\n",
    "interactionsTest = interactions[nTrain:]\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_split = int(len(interactionsTrain) * 0.8)\n",
    "interactionsTrainSplit = interactionsTrain[:train_split]\n",
    "interactionsVal = interactionsTrain[train_split:]\n",
    "# Baselines (original code)\n",
    "# Global average\n",
    "globalAvg = sum([rating for _, _, rating in interactionsTrain]) / len(interactionsTrain)\n",
    "MSE = 0\n",
    "for user, recipe, rating in interactionsTest:\n",
    "    error = (rating - globalAvg) ** 2\n",
    "    MSE += error\n",
    "MSE /= len(interactionsTest)\n",
    "print(\"Test MSE (Using Global Averages) = \" + str(MSE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4eaed8-d1fd-4b96-b72a-07087d84b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (Using User Averages) = 1.5925926749585777\n"
     ]
    }
   ],
   "source": [
    "# User averages\n",
    "recipesPerUser = defaultdict(list)\n",
    "usersPerRecipe = defaultdict(list)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerRecipe = defaultdict(list)\n",
    "for user, recipe, rating in interactionsTrain:\n",
    "    recipesPerUser[user].append(recipe)\n",
    "    usersPerRecipe[recipe].append(user)\n",
    "    ratingsPerUser[user].append(rating)\n",
    "    ratingsPerRecipe[recipe].append(rating)\n",
    "\n",
    "MSE = 0\n",
    "for user, recipe, rating in interactionsTest:\n",
    "    if user in recipesPerUser:\n",
    "        avgUserRating = sum(ratingsPerUser[user]) / len(ratingsPerUser[user])\n",
    "        error = (rating - avgUserRating) ** 2\n",
    "    else:\n",
    "        error = (rating - globalAvg) ** 2\n",
    "    MSE += error\n",
    "MSE /= len(interactionsTest)\n",
    "print(\"Test MSE (Using User Averages) = \" + str(MSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f34471-2320-4bed-a164-5d646eefa037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Factor Model (original code)\n",
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        self.alpha = tf.cast(tf.Variable(mu), dtype=tf.float32)\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)], stddev=0.001))\n",
    "        self.betaR = tf.Variable(tf.random.normal([len(recipeIDs)], stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs), K], stddev=0.001))\n",
    "        self.gammaR = tf.Variable(tf.random.normal([len(recipeIDs), K], stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def predictSample(self, sampleU, sampleR):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_r = tf.nn.embedding_lookup(self.betaR, r)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_r = tf.nn.embedding_lookup(self.gammaR, r)\n",
    "        prediction = self.alpha + beta_u + beta_r + tf.reduce_sum(tf.multiply(gamma_u, gamma_r), 1)\n",
    "        return prediction\n",
    "\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) + tf.reduce_sum(self.betaR**2) + tf.reduce_sum(self.gammaU**2) + tf.reduce_sum(self.gammaR**2))\n",
    "    \n",
    "    def call(self, sampleUser, sampleRecipe, sampleRating):\n",
    "        prediction = self.predictSample(sampleUser, sampleRecipe)\n",
    "        rating = tf.convert_to_tensor(sampleRating, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(prediction - rating) / len(sampleRating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94e037d-69d7-4591-88fa-af460bc3e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step (original code)\n",
    "def trainingStep(model, interactions, optimizer):\n",
    "    Nsamples = 250000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleUser, sampleRecipe, sampleRating = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            user, recipe, rating = random.choice(interactions)\n",
    "            sampleUser.append(userIDs[user])\n",
    "            sampleRecipe.append(recipeIDs[recipe])\n",
    "            sampleRating.append(rating)\n",
    "\n",
    "        loss = model(sampleUser, sampleRecipe, sampleRating)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for (grad, var) in zip(gradients, model.trainable_variables) if grad is not None)\n",
    "    return loss.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63d13f78-245f-4f5e-9d2f-404cb29a0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: Split training data for validation\n",
    "train_split = int(len(interactionsTrain) * 0.8)\n",
    "interactionsTrainSplit = interactionsTrain[:train_split]\n",
    "interactionsVal = interactionsTrain[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d269fcc8-7e59-444a-b594-6b667da9d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: Hyperparameter tuning\n",
    "# def grid_search(interactionsTrain, interactionsVal, userIDs, recipeIDs, globalAvg, K_values, lamb_values, lr_values, num_iterations=50):\n",
    "#     best_params = None\n",
    "#     best_val_MSE = float('inf')\n",
    "    \n",
    "#     for K in K_values:\n",
    "#         for lamb in lamb_values:\n",
    "#             for lr in lr_values:\n",
    "#                 print(f\"Testing K={K}, λ={lamb}, learning_rate={lr}\")\n",
    "#                 optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#                 model = LatentFactorModel(globalAvg, K, lamb)\n",
    "#                 for i in range(num_iterations):\n",
    "#                     loss = trainingStep(model, interactionsTrain)\n",
    "#                     if i % 10 == 0:\n",
    "#                         print(f\"Iteration {i+1}, loss: {loss}\")\n",
    "                \n",
    "#                 val_MSE = 0\n",
    "#                 val_predictions = model.predictSample(\n",
    "#                     [userIDs[interaction[0]] for interaction in interactionsVal],\n",
    "#                     [recipeIDs[interaction[1]] for interaction in interactionsVal]\n",
    "#                 )\n",
    "#                 for i in range(len(interactionsVal)):\n",
    "#                     val_MSE += (interactionsVal[i][2] - val_predictions[i]) ** 2\n",
    "#                 val_MSE /= len(interactionsVal)\n",
    "#                 print(f\"Validation MSE for K={K}, λ={lamb}, lr={lr}: {val_MSE}\")\n",
    "#                 if val_MSE < best_val_MSE:\n",
    "#                     best_val_MSE = val_MSE\n",
    "#                     best_params = (K, lamb, lr)\n",
    "    \n",
    "#     return best_params, best_val_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6127203-fd6e-4df3-8321-169e23b7e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split = int(len(interactionsTrain) * 0.8)\n",
    "# interactionsTrainSplit = interactionsTrain[:train_split]\n",
    "# interactionsVal = interactionsTrain[train_split:]\n",
    "\n",
    "def grid_search(interactionsTrain, interactionsVal, userIDs, recipeIDs, globalAvg, K_values, lamb_values, lr_values, num_iterations=10):\n",
    "    best_params = None\n",
    "    best_val_MSE = float('inf')\n",
    "    \n",
    "    for K in K_values:\n",
    "        for lamb in lamb_values:\n",
    "            for lr in lr_values:\n",
    "                print(f\"Testing K={K}, λ={lamb}, learning_rate={lr}\")\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                model = LatentFactorModel(globalAvg, K, lamb)\n",
    "                for i in range(num_iterations):\n",
    "                    loss = trainingStep(model, interactionsTrain, optimizer)  # Pass optimizer here\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\"Iteration {i+1}, loss: {loss}\")\n",
    "                \n",
    "                val_MSE = 0\n",
    "                val_predictions = model.predictSample(\n",
    "                    [userIDs[interaction[0]] for interaction in interactionsVal],\n",
    "                    [recipeIDs[interaction[1]] for interaction in interactionsVal]\n",
    "                )\n",
    "                for i in range(len(interactionsVal)):\n",
    "                    val_MSE += (interactionsVal[i][2] - val_predictions[i]) ** 2\n",
    "                val_MSE /= len(interactionsVal)\n",
    "                print(f\"Validation MSE for K={K}, λ={lamb}, lr={lr}: {val_MSE}\")\n",
    "                if val_MSE < best_val_MSE:\n",
    "                    best_val_MSE = val_MSE\n",
    "                    best_params = (K, lamb, lr)\n",
    "    \n",
    "    return best_params, best_val_MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8513ae-2d3d-4375-aba2-588b5965d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing K=8, λ=0.0001, learning_rate=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 06:06:53.255731: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss: 0.7943146228790283\n",
      "Validation MSE for K=8, λ=0.0001, lr=0.01: 1.564568042755127\n",
      "Testing K=8, λ=0.0001, learning_rate=0.05\n",
      "Iteration 1, loss: 0.8058547973632812\n",
      "Validation MSE for K=8, λ=0.0001, lr=0.05: 1.5292800664901733\n",
      "Testing K=8, λ=0.0001, learning_rate=0.1\n",
      "Iteration 1, loss: 0.7970572710037231\n",
      "Validation MSE for K=8, λ=0.0001, lr=0.1: 1.5359610319137573\n",
      "Testing K=8, λ=0.001, learning_rate=0.01\n",
      "Iteration 1, loss: 0.8078670501708984\n",
      "Validation MSE for K=8, λ=0.001, lr=0.01: 1.581279993057251\n",
      "Testing K=8, λ=0.001, learning_rate=0.05\n",
      "Iteration 1, loss: 0.8016877174377441\n",
      "Validation MSE for K=8, λ=0.001, lr=0.05: 1.5717825889587402\n",
      "Testing K=8, λ=0.001, learning_rate=0.1\n",
      "Iteration 1, loss: 0.8042346835136414\n",
      "Validation MSE for K=8, λ=0.001, lr=0.1: 1.5722134113311768\n",
      "Testing K=8, λ=0.01, learning_rate=0.01\n",
      "Iteration 1, loss: 0.836323082447052\n",
      "Validation MSE for K=8, λ=0.01, lr=0.01: 1.5968083143234253\n",
      "Testing K=8, λ=0.01, learning_rate=0.05\n",
      "Iteration 1, loss: 0.840681791305542\n",
      "Validation MSE for K=8, λ=0.01, lr=0.05: 1.5923349857330322\n",
      "Testing K=8, λ=0.01, learning_rate=0.1\n",
      "Iteration 1, loss: 0.8482925891876221\n",
      "Validation MSE for K=8, λ=0.01, lr=0.1: 1.5856728553771973\n",
      "Best parameters: K=8, λ=0.0001, learning_rate=0.05\n",
      "Best Validation MSE: 1.5292800664901733\n"
     ]
    }
   ],
   "source": [
    "# Define parameter ranges\n",
    "# Define parameter ranges for grid search\n",
    "K_values = [8]\n",
    "lamb_values = [1e-4, 1e-3, 1e-2]\n",
    "lr_values = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Run grid search\n",
    "best_params, best_val_MSE = grid_search(\n",
    "    interactionsTrainSplit, \n",
    "    interactionsVal, \n",
    "    userIDs, \n",
    "    recipeIDs, \n",
    "    globalAvg, \n",
    "    K_values, \n",
    "    lamb_values, \n",
    "    lr_values\n",
    ")\n",
    "\n",
    "print(f\"Best parameters: K={best_params[0]}, λ={best_params[1]}, learning_rate={best_params[2]}\")\n",
    "print(f\"Best Validation MSE: {best_val_MSE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dca8f7c-85b5-4921-a6d3-fef42d2ab68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss: 0.7909595370292664\n",
      "Iteration 11, loss: 0.7872207164764404\n",
      "Iteration 21, loss: 0.7802796363830566\n",
      "Iteration 31, loss: 0.7818827033042908\n",
      "Iteration 41, loss: 0.7761270403862\n",
      "Iteration 51, loss: 0.7837302684783936\n",
      "Iteration 61, loss: 0.7791503071784973\n",
      "Iteration 71, loss: 0.7825021743774414\n",
      "Iteration 81, loss: 0.7894480228424072\n",
      "Iteration 91, loss: 0.7632641792297363\n"
     ]
    }
   ],
   "source": [
    "# Retrain with best parameters\n",
    "final_model = LatentFactorModel(globalAvg, best_params[0], best_params[1])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_params[2])\n",
    "\n",
    "for i in range(100):\n",
    "    loss = trainingStep(final_model, interactionsTrain, optimizer)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i+1}, loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e638c72-bd8f-43b7-a946-aaf259737d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 1.538376808166504\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_MSE = 0\n",
    "test_predictions = final_model.predictSample(\n",
    "    [userIDs[interaction[0]] for interaction in interactionsTest],\n",
    "    [recipeIDs[interaction[1]] for interaction in interactionsTest]\n",
    ")\n",
    "\n",
    "for i in range(len(interactionsTest)):\n",
    "    test_MSE += (interactionsTest[i][2] - test_predictions[i]) ** 2\n",
    "test_MSE /= len(interactionsTest)\n",
    "\n",
    "print(f\"Final Test MSE: {test_MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de919fc-ac04-46b7-8fcf-f32cb879630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 0.0001, 0.05)\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
